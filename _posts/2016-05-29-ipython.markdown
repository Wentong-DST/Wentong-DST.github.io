
# Saving model weights

## Description

Short demonstration on how to save model weights using lasagne package. 

1st Method: Raises error when transferring between GPU (cuda) and non-GPU systems.

2ns Method: Compatible between GPU (cuda) and non-GPU systems. Might require slight modification for more complex configurations.

Dataset: MNIST handwritten digits.

## References

[1] Pickle python lasagne model, http://stackoverflow.com/questions/34338838/pickle-python-lasagne-model

[2] lasagne.layers -- Lasagne 0.2.dev1 documentation, http://lasagne.readthedocs.io/en/latest/modules/layers.html

#### Model etc:

Here I am just re-using one of the models in 02Lasagne.ipynb. 


```python
import os
import pickle
import gzip
from urllib import urlretrieve

import numpy as np
import theano
import theano.tensor as T
import lasagne


# data
filename = "mnist.pkl.gz"
if not os.path.exists(filename):
    url = "http://deeplearning.net/data/mnist/mnist.pkl.gz"
    urlretrieve(url, filename)
train, val, test = pickle.load(gzip.open(filename, "rb"))
X_train, y_train = train
X_val, y_val = val
X_test, y_test = test


# layers
inputlayer = lasagne.layers.InputLayer((None, 784)) # None: variable batch size, 784: input vector dimension
shapelayer = lasagne.layers.ReshapeLayer(inputlayer,
                                         (-1, 1, 28, 28))
conv2dlayer = lasagne.layers.Conv2DLayer(shapelayer,
                                         num_filters=5,
                                         filter_size=5,
                                         pad=1)
outputlayer = lasagne.layers.DenseLayer(shapelayer, # preceding layer
                                        num_units=10, # number of units/neurons
                                        nonlinearity=lasagne.nonlinearities.softmax) # activation function
# symbolics

# symbolics
X = T.matrix()
Y = T.ivector()
output = lasagne.layers.get_output(outputlayer,X)
prediction = output.argmax(-1)

# metrics
lossfunction = T.mean(lasagne.objectives.categorical_crossentropy(output, Y))
accuracy = T.mean(T.eq(prediction, Y))
parameters = lasagne.layers.get_all_params(outputlayer)
print "parameters {}".format(parameters)

# optimizations
gradient = T.grad(cost=lossfunction, wrt=parameters)
updates = lasagne.updates.sgd(gradient,
                              parameters, 
                              learning_rate=0.5)

# functions
training_function = theano.function([X, Y], 
                                    [lossfunction, accuracy], 
                                    updates=updates)

validation_function = theano.function([X, Y], 
                                      [lossfunction, accuracy])

prediction_function = theano.function([X], 
                                      prediction)


def train(epochs=20):
    """
    Training procedure (written by myself, so probably not the best method):
        * take random 64-sample from training data set
        * SGD on sample
        * repeat (this counts as one epoch)
    """
    import time
    print "{:<10}{:<20}{:<20}{:<20}{:<20}{:<20}".format("Epoch", 
                                                        "TL", "TA", 
                                                        "VL", "VA",
                                                        "Time")
    for e in range(epochs):
        t1 = time.time()
        idx = np.random.choice(len(X_train),64)
        x_sample = [X_train[i] for i in idx]
        y_sample = [y_train[i] for i in idx]
        tl, ta = training_function(X_train.astype('float32'), y_train.astype('int32'))
        vl, va = validation_function(X_val.astype('float32'), y_val.astype('int32'))
        t2 = time.time()
        print "{:<10}{:<20}{:<20}{:<20}{:<20}{:<20}".format(e+1, tl, ta, vl, va, t2-t1)
    return None


def performance():
    """
    Performance over test data
    """
    P = prediction_function(test[0])
    ans = test[1]
    correct, wrong = 0, 0
    for p, a in zip(P,ans):
        #print "Machine prediction: {}, Actual: {}.".format(p,a)
        if p == a:
            correct += 1
        else:
            wrong += 1
    print "Performance on test data = {} / {}.".format(correct, correct + wrong)
    return None

train()
performance()
```

    parameters [W, b]
    Epoch     TL                  TA                  VL                  VA                  Time                
    1         2.40228777705       0.09274             1.88100445888       0.4164              1.48112392426       
    2         1.89849303895       0.40066             1.53739384386       0.6415              0.928575992584      
    3         1.5594330159        0.6223              1.30698162089       0.7624              0.69598197937       
    4         1.33796026736       0.73502             1.15034258032       0.7727              0.663397073746      
    5         1.18595463436       0.74864             1.03402575632       0.8188              0.660306930542      
    6         1.0739169622        0.79066             0.948251105654      0.814               0.721266031265      
    7         0.990556491876      0.79104             0.88148045634       0.8379              0.640022993088      
    8         0.92594580254       0.81152             0.828505505327      0.835               0.722093105316      
    9         0.874310453265      0.81358             0.78533158364       0.8501              0.639857053757      
    10        0.832304698893      0.82448             0.749095061746      0.8484              0.715982913971      
    11        0.796820801847      0.8273              0.718542494367      0.8572              0.725342988968      
    12        0.766848966241      0.83328             0.692161924751      0.8569              0.73318696022       
    13        0.740892220381      0.83658             0.66936952024       0.8614              0.725602149963      
    14        0.718337433739      0.8398              0.64927170863       0.8623              0.738201141357      
    15        0.698452833577      0.84266             0.631561875224      0.866               0.701247930527      
    16        0.680794112132      0.84506             0.615671492079      0.8665              0.69540810585       
    17        0.664966937861      0.8472              0.601435357223      0.8692              0.740391969681      
    18        0.650681367706      0.84932             0.58849960728       0.8701              0.66308093071       
    19        0.63770320471       0.85134             0.57675260864       0.8718              0.795945167542      
    20        0.625847402843      0.8526              0.565981216354      0.8733              0.779918909073      
    Performance on test data = 8684 / 10000.


# 1st method

#### Saving weights:

Remark(s): 
* This requires only numpy and the file will be saved using extension .npz.
* Raises error if you're transferring file between GPU (cuda) and non-GPU systems.


```python
np.savez('03SavingWeights.npz', *lasagne.layers.get_all_param_values(outputlayer))
```

#### Retrieving the weights:


```python
saved_weights = np.load('03SavingWeights.npz')
print saved_weights.files
saved_weights = [saved_weights["arr_{}".format(i)]
                 for i in range(len(saved_weights.files))]
```

    ['arr_1', 'arr_0']


#### Display weights for comparison:


```python
saved_weights
```




    [array([[-0.03249001, -0.07045451, -0.02814807, ..., -0.05284556,
              0.03690574, -0.06013901],
            [-0.00842832,  0.01964517, -0.05776376, ...,  0.05986743,
             -0.0703351 , -0.0132368 ],
            [-0.01298645, -0.08096745, -0.05045535, ..., -0.01248548,
              0.04793729, -0.0424201 ],
            ..., 
            [-0.03641444, -0.0407797 , -0.05780418, ...,  0.03900748,
             -0.00059478,  0.07610522],
            [ 0.04855424,  0.06123881,  0.03424219, ...,  0.02216629,
             -0.01446273,  0.06985327],
            [ 0.00354903,  0.01181965,  0.02254689, ...,  0.06372722,
              0.02320013,  0.07739303]]),
     array([-0.06060272,  0.09587689, -0.01142803, -0.03617392,  0.03668426,
             0.05739763, -0.0122143 ,  0.05652771, -0.11284842, -0.01321909])]




```python
lasagne.layers.get_all_param_values(outputlayer)
```




    [array([[-0.03249001, -0.07045451, -0.02814807, ..., -0.05284556,
              0.03690574, -0.06013901],
            [-0.00842832,  0.01964517, -0.05776376, ...,  0.05986743,
             -0.0703351 , -0.0132368 ],
            [-0.01298645, -0.08096745, -0.05045535, ..., -0.01248548,
              0.04793729, -0.0424201 ],
            ..., 
            [-0.03641444, -0.0407797 , -0.05780418, ...,  0.03900748,
             -0.00059478,  0.07610522],
            [ 0.04855424,  0.06123881,  0.03424219, ...,  0.02216629,
             -0.01446273,  0.06985327],
            [ 0.00354903,  0.01181965,  0.02254689, ...,  0.06372722,
              0.02320013,  0.07739303]]),
     array([-0.06060272,  0.09587689, -0.01142803, -0.03617392,  0.03668426,
             0.05739763, -0.0122143 ,  0.05652771, -0.11284842, -0.01321909])]



#### Updating model weights:


```python
lasagne.layers.set_all_param_values(outputlayer,saved_weights)
```


```python
lasagne.layers.get_all_param_values(outputlayer)
```




    [array([[-0.03249001, -0.07045451, -0.02814807, ..., -0.05284556,
              0.03690574, -0.06013901],
            [-0.00842832,  0.01964517, -0.05776376, ...,  0.05986743,
             -0.0703351 , -0.0132368 ],
            [-0.01298645, -0.08096745, -0.05045535, ..., -0.01248548,
              0.04793729, -0.0424201 ],
            ..., 
            [-0.03641444, -0.0407797 , -0.05780418, ...,  0.03900748,
             -0.00059478,  0.07610522],
            [ 0.04855424,  0.06123881,  0.03424219, ...,  0.02216629,
             -0.01446273,  0.06985327],
            [ 0.00354903,  0.01181965,  0.02254689, ...,  0.06372722,
              0.02320013,  0.07739303]]),
     array([-0.06060272,  0.09587689, -0.01142803, -0.03617392,  0.03668426,
             0.05739763, -0.0122143 ,  0.05652771, -0.11284842, -0.01321909])]



# 2nd method

Remarks:
* No error between GPU (cuda) and non-GPU systems
* Avoids error by converting weight components to numpy array first.
* Might require modification for more complex configurations.


```python
weights = lasagne.layers.get_all_param_values(outputlayer) # extract weights
```


```python
weights = [np.array(w) for w in weights] # conversion to numpy array
```


```python
# store weights using pickle
f = gzip.open("03SavingWeights.pickle.gz", "wb")
pickle.dump(weights, f)
f.close()
```


```python
# retrieve weights using pickle
f = gzip.open("03SavingWeights.pickle.gz", "rb")
saved_weights = pickle.load(f)
lasagne.layers.set_all_param_values(outputlayer, saved_weights)
```


```python
lasagne.layers.get_all_param_values(outputlayer)
```




    [array([[-0.03249001, -0.07045451, -0.02814807, ..., -0.05284556,
              0.03690574, -0.06013901],
            [-0.00842832,  0.01964517, -0.05776376, ...,  0.05986743,
             -0.0703351 , -0.0132368 ],
            [-0.01298645, -0.08096745, -0.05045535, ..., -0.01248548,
              0.04793729, -0.0424201 ],
            ..., 
            [-0.03641444, -0.0407797 , -0.05780418, ...,  0.03900748,
             -0.00059478,  0.07610522],
            [ 0.04855424,  0.06123881,  0.03424219, ...,  0.02216629,
             -0.01446273,  0.06985327],
            [ 0.00354903,  0.01181965,  0.02254689, ...,  0.06372722,
              0.02320013,  0.07739303]]),
     array([-0.06060272,  0.09587689, -0.01142803, -0.03617392,  0.03668426,
             0.05739763, -0.0122143 ,  0.05652771, -0.11284842, -0.01321909])]


